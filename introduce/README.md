# How accurate can AI generate SQL? AIç”ŸæˆSQLæœ‰å¤šå‡†ç¡®ï¼Ÿ

Published 2023-08-17 å‘å¸ƒäº2023å¹´8æœˆ17æ—¥<br>

ğŸ”¥ğŸ”¥ğŸ”¥It turns out that it can do REALLY well if given the right context å¦‚æœæä¾›äº†æ­£ç¡®çš„ä¸Šä¸‹æ–‡ï¼Œç»“æœä¼šéå¸¸å¥½<br>

AI SQL Accuracy: Testing different LLMs + context strategies to maximize SQL generation accuracy<br>

AI SQLå‡†ç¡®æ€§ï¼šæµ‹è¯•ä¸åŒçš„LLMs + ä¸Šä¸‹æ–‡ç­–ç•¥ä»¥æœ€å¤§åŒ–SQLç”Ÿæˆçš„å‡†ç¡®æ€§<br>
- [How accurate can AI generate SQL? AIç”ŸæˆSQLæœ‰å¤šå‡†ç¡®ï¼Ÿ](#how-accurate-can-ai-generate-sql-aiç”Ÿæˆsqlæœ‰å¤šå‡†ç¡®)
  - [TLDR ç®€è€Œè¨€ä¹‹](#tldr-ç®€è€Œè¨€ä¹‹)
  - [Why use AI to generate SQL?](#why-use-ai-to-generate-sql)
  - [è¡¥å……:providing contextually relevant correct SQLè§£é‡Š](#è¡¥å……providing-contextually-relevant-correct-sqlè§£é‡Š)
  - [Setting up architecture of the test(è®¾ç½®æµ‹è¯•æ¶æ„)](#setting-up-architecture-of-the-testè®¾ç½®æµ‹è¯•æ¶æ„)
    - [1. Question - We start with the business question.(æˆ‘ä»¬ä»ä¸šåŠ¡é—®é¢˜å¼€å§‹)](#1-question---we-start-with-the-business-questionæˆ‘ä»¬ä»ä¸šåŠ¡é—®é¢˜å¼€å§‹)
    - [2. Prompt - We create the prompt to send to the LLM.(æˆ‘ä»¬åˆ›å»ºé€å…¥LLMçš„prompt)](#2-prompt---we-create-the-prompt-to-send-to-the-llmæˆ‘ä»¬åˆ›å»ºé€å…¥llmçš„prompt)
    - [3. Generate SQL - Using an API, weâ€™ll send the prompt to the LLM and get back generated SQL.](#3-generate-sql---using-an-api-well-send-the-prompt-to-the-llm-and-get-back-generated-sql)
    - [4. Run SQL - We'll run the SQL against the database.](#4-run-sql---well-run-the-sql-against-the-database)
    - [5. Validate results - éªŒè¯ç»“æœ](#5-validate-results---éªŒè¯ç»“æœ)
  - [Setting up the test levers(è®¾ç½®æµ‹è¯•æ æ†)](#setting-up-the-test-leversè®¾ç½®æµ‹è¯•æ æ†)
    - [Choosing a dataset(é€‰æ‹©æ•°æ®é›†)](#choosing-a-dataseté€‰æ‹©æ•°æ®é›†)
    - [Choosing the questions](#choosing-the-questions)
    - [Choosing the prompt(é€‰æ‹©æç¤ºè¯­)](#choosing-the-prompté€‰æ‹©æç¤ºè¯­)
    - [Choosing the LLMs (Foundational models) - é€‰æ‹©LLMsï¼ˆåŸºç¡€æ¨¡å‹ï¼‰](#choosing-the-llms-foundational-models---é€‰æ‹©llmsåŸºç¡€æ¨¡å‹)
    - [Choosing the context(é€‰æ‹©ä¸Šä¸‹æ–‡)](#choosing-the-contexté€‰æ‹©ä¸Šä¸‹æ–‡)
    - [é™æ€SQLä¸åŠ¨æ€SQLç¤ºä¾‹:](#é™æ€sqlä¸åŠ¨æ€sqlç¤ºä¾‹)
  - [Using ChatGPT to generate SQL(ä½¿ç”¨ ChatGPT ç”Ÿæˆ SQL)](#using-chatgpt-to-generate-sqlä½¿ç”¨-chatgpt-ç”Ÿæˆ-sql)
    - [Prompt:](#prompt)
    - [Response:](#response)
    - [Using schema only:](#using-schema-only)
    - [Using SQL examples(ä½¿ç”¨ SQL ç¤ºä¾‹)](#using-sql-examplesä½¿ç”¨-sql-ç¤ºä¾‹)
  - [VANNA\_API\_KEY ç”¨åœ¨å“ªé‡Œï¼Ÿ](#vanna_api_key-ç”¨åœ¨å“ªé‡Œ)

## TLDR ç®€è€Œè¨€ä¹‹

The promise of having an autonomous(è‡ªä¸»çš„) AI agent that can answer business usersâ€™ plain(æ™®é€šçš„) English questions is an attractive but thus far elusive proposition.<br>

æ‹¥æœ‰ä¸€ä¸ªèƒ½å¤Ÿå›ç­”å•†ä¸šç”¨æˆ·æ™®é€šè‹±è¯­é—®é¢˜çš„è‡ªä¸»AIä»£ç†çš„æ‰¿è¯ºæ˜¯ä¸€ä¸ªå¸å¼•äººä½†è¿„ä»Šä¸ºæ­¢éš¾ä»¥å®ç°çš„ä¸»å¼ ã€‚<br>

Many have tried, with limited success, to get ChatGPT to write.<br>

è®¸å¤šäººå°è¯•è¿‡ï¼Œä½†ChatGPTå†™ä½œçš„æˆåŠŸç‡æœ‰é™ã€‚<br>

The failure is primarily due of a lack of the LLM's knowledge of the particular dataset itâ€™s being asked to query.<br>

å¤±è´¥ä¸»è¦æ˜¯ç”±äºLLMå¯¹å…¶è¢«è¦æ±‚æŸ¥è¯¢çš„ç‰¹å®šæ•°æ®é›†çš„çŸ¥è¯†ä¸è¶³æ‰€è‡´ã€‚<br>

ğŸ”¥In this paper, we show that **context is everything**, and with the right context, we can **get from ~3% accuracy to ~80% accuracy**.<br>

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ä¸Šä¸‹æ–‡è‡³å…³é‡è¦ï¼Œå¹¶ä¸”é€šè¿‡æ­£ç¡®çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬å¯ä»¥ä»å¤§çº¦3%çš„å‡†ç¡®ç‡æé«˜åˆ°å¤§çº¦80%çš„å‡†ç¡®ç‡ã€‚<br>

We go through three different context strategies, and showcase one that is the clear winner - where we combine schema definitions, documentation, and prior SQL queries with a relevance search.<br>

æˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§ä¸åŒçš„ä¸Šä¸‹æ–‡ç­–ç•¥ï¼Œå¹¶å±•ç¤ºäº†ä¸€ä¸ªæ˜æ˜¾çš„ä¼˜èƒœè€… - åœ¨è¿™ç§ç­–ç•¥ä¸­ï¼Œæˆ‘ä»¬å°†æ¨¡å¼å®šä¹‰ã€æ–‡æ¡£å’Œå…ˆå‰çš„SQLæŸ¥è¯¢ä¸ç›¸å…³æ€§æœç´¢ç›¸ç»“åˆã€‚<br>

> prior SQL queries æ¨æµ‹è¡¨ç¤ºçš„æ˜¯é¢„å…ˆè®¾ç½®çš„SQLè¯­å¥ã€‚

We also compare a few different LLMs - including Google Bison, GPT 3.5, GPT 4, and a brief attempt with Llama 2.<br>

æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†å‡ ç§ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬Google Bisonã€GPT 3.5ã€GPT 4ä»¥åŠç®€çŸ­å°è¯•äº†Llama 2ã€‚<br>

ğŸ”¥While GPT 4 takes the crown of the best overall LLM for generating SQL, Googleâ€™s Bison is roughly equivalent when enough context is provided.<br>

ğŸ”¥å½“æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ—¶ï¼Œå°½ç®¡GPT 4åœ¨ç”ŸæˆSQLæ–¹é¢è¢«å…¬è®¤ä¸ºæœ€ä¼˜ç§€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒGoogleçš„Bisonä¹Ÿä¸ä¹‹å¤§è‡´ç›¸å½“ã€‚<br>

Finally, we show how you can use the methods demonstrated here to generate SQL for your database.<br>

æœ€åï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨è¿™é‡Œæ¼”ç¤ºçš„æ–¹æ³•ä¸ºæ‚¨çš„æ•°æ®åº“ç”ŸæˆSQLã€‚<br>

Here's a summary of our key findings - è¿™é‡Œæ˜¯æˆ‘ä»¬å…³é”®å‘ç°çš„æ€»ç»“ -<br>

![](./sql_gen_1.jpg)


## Why use AI to generate SQL?

Many organizations have now adopted some sort of data warehouse or data lake - a repository of a lot of the organizationâ€™s critical data that is queryable for analytical purposes.<br>

è®¸å¤šç»„ç»‡ç°åœ¨å·²ç»é‡‡ç”¨äº†æŸç§å½¢å¼çš„æ•°æ®ä»“åº“æˆ–æ•°æ®æ¹–â€”â€”ä¸€ä¸ªåŒ…å«å¤§é‡ç»„ç»‡å…³é”®æ•°æ®çš„å­˜å‚¨åº“ï¼Œå¯ç”¨äºåˆ†æç›®çš„æŸ¥è¯¢ã€‚<br>

This ocean of data is brimming with potential insights, but only a small fraction of people in an enterprise have the two skills required to harness the data â€”<br>

è¿™æµ·é‡çš„æ•°æ®å……æ»¡äº†æ½œåœ¨çš„æ´å¯ŸåŠ›ï¼Œä½†ä¼ä¸šä¸­åªæœ‰å°‘æ•°äººå…·å¤‡åˆ©ç”¨è¿™äº›æ•°æ®æ‰€éœ€çš„ä¸¤é¡¹æŠ€èƒ½â€”â€”<br>

1. A solid comprehension of advanced SQL, and / å¯¹é«˜çº§SQLæœ‰æ‰å®çš„ç†è§£ï¼Œå¹¶ä¸”

2. A comprehensive knowledge of the organizationâ€™s unique data structure & schema / å¯¹ç»„ç»‡ç‹¬ç‰¹çš„æ•°æ®ç»“æ„å’Œæ¨¡å¼æœ‰å…¨é¢çš„äº†è§£

The number of people with both of the above is not only vanishingly small, but likely not the same people that have the majority of the questions.<br>

å…·å¤‡ä¸Šè¿°ä¸¤ç§èƒ½åŠ›çš„äººæ•°ä¸ä»…æå°‘ï¼Œè€Œä¸”å¾ˆå¯èƒ½ä¸æ˜¯é‚£äº›æœ‰å¤§éƒ¨åˆ†ç–‘é—®çš„äººã€‚<br>

So what actually happens inside organizations? <br>

é‚£ä¹ˆï¼Œç»„ç»‡å†…éƒ¨å®é™…ä¸Šå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ<br>

Business users, like product managers, sales managers, and executives, have data questions that will inform business decisions and strategy.<br>

ä¸šåŠ¡ç”¨æˆ·ï¼Œå¦‚äº§å“ç»ç†ã€é”€å”®ç»ç†å’Œé«˜ç®¡ï¼Œä¼šæœ‰ä¸€äº›æ•°æ®é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å°†ç”¨äºæŒ‡å¯¼ä¸šåŠ¡å†³ç­–å’Œæˆ˜ç•¥ã€‚<br>

Theyâ€™ll first check dashboards, but most questions are ad hoc and specific, and the answers arenâ€™t available, so theyâ€™ll ask a data analyst or engineer - whomever possesses the combination of skills above.<br>

ä»–ä»¬é¦–å…ˆä¼šæŸ¥çœ‹ä»ªè¡¨æ¿ï¼Œä½†å¤§å¤šæ•°é—®é¢˜éƒ½æ˜¯ç‰¹å®šçš„ä¸´æ—¶é—®é¢˜ï¼Œç­”æ¡ˆé€šå¸¸ä¸å¯å¾—ï¼Œå› æ­¤ä»–ä»¬ä¼šå‘æ•°æ®åˆ†æå¸ˆæˆ–å·¥ç¨‹å¸ˆæé—®â€”â€”å³ä»»ä½•æ‹¥æœ‰ä¸Šè¿°æŠ€èƒ½ç»„åˆçš„äººã€‚<br>

These people are busy, and take a while to get to the request, and as soon as they get an answer, the business user has follow up questions.<br>

è¿™äº›äººå¾ˆå¿™ï¼Œå¤„ç†è¿™äº›è¯·æ±‚éœ€è¦ä¸€äº›æ—¶é—´ï¼Œä¸€æ—¦ä»–ä»¬å¾—å‡ºç­”æ¡ˆï¼Œä¸šåŠ¡ç”¨æˆ·åˆæœ‰äº†åç»­é—®é¢˜ã€‚<br>

This process is painful for both the business user (long lead times to get answers) and the analyst (distracts from their main projects), and leads to many potential insights being lost.<br>

è¿™ä¸ªè¿‡ç¨‹å¯¹ä¸šåŠ¡ç”¨æˆ·ï¼ˆè·å–ç­”æ¡ˆçš„æ—¶é—´è¿‡é•¿ï¼‰å’Œåˆ†æå¸ˆï¼ˆåˆ†æ•£äº†ä»–ä»¬ä¸»è¦é¡¹ç›®çš„æ³¨æ„åŠ›ï¼‰éƒ½æ˜¯ç—›è‹¦çš„ï¼Œè€Œä¸”ä¼šå¯¼è‡´è®¸å¤šæ½œåœ¨çš„æ´å¯Ÿä¸¢å¤±ã€‚<br>

![](./ä¸šåŠ¡æµç¨‹.png)

Enter generative AI! LLMs potentially give the opportunity to business users to query the database in plain English (**with the LLMs doing the SQL translation**), and we have heard from dozens of companies that this would be a game changer for their data teams and even their businesses.<br>

é”®å…¥ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½ä¸ºå•†ä¸šç”¨æˆ·æä¾›ç”¨ç®€å•è‹±è¯­æŸ¥è¯¢æ•°æ®åº“çš„æœºä¼šï¼ˆ**ç”±å¤§å‹è¯­è¨€æ¨¡å‹å®ŒæˆSQLç¿»è¯‘**ï¼‰ï¼Œæˆ‘ä»¬å·²ç»ä»æ•°åå®¶å…¬å¸å¬è¯´è¿™å°†æ˜¯å…¶æ•°æ®å›¢é˜Ÿä¹ƒè‡³æ•´ä¸ªä¸šåŠ¡çš„æ¸¸æˆè§„åˆ™æ”¹å˜è€…ã€‚<br>

The key challenge is generating accurate SQL for complex and messy databases.<br>

å…³é”®æŒ‘æˆ˜æ˜¯ä¸ºå¤æ‚å’Œæ··ä¹±çš„æ•°æ®åº“ç”Ÿæˆå‡†ç¡®çš„SQLã€‚<br>

ğŸš¨Plenty of people weâ€™ve spoken with have tried to use ChatGPT to write SQL with limited success and a lot of pain.<br>

ğŸš¨æˆ‘ä»¬ä¸è®¸å¤šäººäº¤è°ˆï¼Œä»–ä»¬å°è¯•ä½¿ç”¨ChatGPTç¼–å†™SQLï¼Œä½†æˆåŠŸæœ‰é™ï¼Œä¸”è¿‡ç¨‹ç—›è‹¦ã€‚<br>

Many have given up and reverted back to the old fashioned way of manually writing SQL.<br>

è®¸å¤šäººå·²æ”¾å¼ƒå¹¶å›å½’åˆ°ä¼ ç»Ÿçš„æ‰‹å·¥ç¼–å†™SQLçš„æ–¹å¼ã€‚<br>

At best, ChatGPT is a sometimes useful co-pilot for analysts to get syntax right.<br>

åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼ŒChatGPTæœ‰æ—¶å¯ä»¥ä½œä¸ºåˆ†æå¸ˆçš„æœ‰ç”¨å‰¯é©¾é©¶ï¼Œå¸®åŠ©ä»–ä»¬æ­£ç¡®ä½¿ç”¨è¯­æ³•ã€‚<br>

But thereâ€™s hope! Weâ€™ve spent the last few months immersed in this problem, trying various models, techniques and approaches to improve the accuracy of SQL generated by LLMs.<br>

ä½†æœ‰å¸Œæœ›ï¼æˆ‘ä»¬åœ¨è¿‡å»å‡ ä¸ªæœˆä¸­æ·±å…¥ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œå°è¯•äº†å¤šç§æ¨¡å‹ã€æŠ€æœ¯å’Œæ–¹æ³•æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆSQLçš„å‡†ç¡®æ€§ã€‚<br>

In this paper, we show the performance of various LLMs and how the strategy of providing contextually relevant correct SQL to the LLM can allow the LLM to achieve extremely high accuracy.<br>

åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å„ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼Œ**ä»¥åŠå¦‚ä½•é€šè¿‡æä¾›ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„æ­£ç¡®SQLç»™å¤§å‹è¯­è¨€æ¨¡å‹**ï¼Œä½¿å…¶èƒ½å¤Ÿå®ç°æé«˜çš„å‡†ç¡®åº¦ã€‚<br>


## è¡¥å……:providing contextually relevant correct SQLè§£é‡Š

![](./gpt_context.jpg)

## Setting up architecture of the test(è®¾ç½®æµ‹è¯•æ¶æ„)

First, we needed to define the architecture of the test.<br>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰æµ‹è¯•çš„æ¶æ„ã€‚<br>

A rough outline is below, in a five step process, with pseudo code below -<br>

ä¸‹é¢æ˜¯ä¸€ä¸ªç²—ç•¥çš„æ¦‚è¿°ï¼Œåˆ†ä¸ºäº”ä¸ªæ­¥éª¤ï¼Œä¸‹é¢æœ‰ä¼ªä»£ç  -<br>

![](./æµ‹è¯•æ¶æ„.jpg)

### 1. Question - We start with the business question.(æˆ‘ä»¬ä»ä¸šåŠ¡é—®é¢˜å¼€å§‹)

```python
# å¾·å›½æœ‰å¤šå°‘å®¢æˆ·ï¼Ÿ
question = "how many clients are there in germany"
```

### 2. Prompt - We create the prompt to send to the LLM.(æˆ‘ä»¬åˆ›å»ºé€å…¥LLMçš„prompt)

```python
prompt = f"""
Write a SQL statement for the following question:
{question}
"""
```

### 3. Generate SQL - Using an API, weâ€™ll send the prompt to the LLM and get back generated SQL.

```python
sql = llm.api(api_key=api_key, prompt=prompt, parameters=parameters)
```

### 4. Run SQL - We'll run the SQL against the database.

```python
df = db.conn.execute(sql)
```

### 5. Validate results - éªŒè¯ç»“æœ  

Finally, weâ€™ll validate that the results are in line with what we expect.<br>

æœ€åï¼Œæˆ‘ä»¬å°†ç¡®è®¤ç»“æœæ˜¯å¦ç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸã€‚<br>

There are some shades of grey when it comes to the results so we did a manual evaluation of the results.<br>

ç»“æœå­˜åœ¨ä¸€äº›ç°è‰²åœ°å¸¦ï¼Œå› æ­¤æˆ‘ä»¬è¿›è¡Œäº†æ‰‹åŠ¨è¯„ä¼°ã€‚<br>

You can see those results [here](https://github.com/vanna-ai/research/blob/main/data/sec_evaluation_data_tagged.csv) - ä½ å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹è¿™äº›ç»“æœã€‚<br>


## Setting up the test levers(è®¾ç½®æµ‹è¯•æ æ†)

Now that we have our experiment set up, weâ€™ll need to figure out what levers would impact accuracy, and what our test set would be.<br>

ç°åœ¨æˆ‘ä»¬å·²ç»å»ºç«‹äº†å®éªŒï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šå“ªäº›æ æ†ä¼šå½±å“å‡†ç¡®æ€§ï¼Œä»¥åŠæˆ‘ä»¬çš„æµ‹è¯•é›†ä¼šæ˜¯ä»€ä¹ˆã€‚<br>

We tried two levers (the LLMs and the training data used), and we ran on 20 questions that made up our test set.<br>

æˆ‘ä»¬å°è¯•äº†ä¸¤ä¸ªæ æ†ï¼ˆä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè®­ç»ƒæ•°æ®ï¼‰ï¼Œå¹¶å¯¹æ„æˆæˆ‘ä»¬æµ‹è¯•é›†çš„20ä¸ªé—®é¢˜è¿›è¡Œäº†æµ‹è¯•ã€‚<br>

So we ran a total of 3 LLMs x 3 context strategies x 20 questions = 180 individual trials in this experiment.<br>

å› æ­¤ï¼Œåœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬æ€»å…±è¿›è¡Œäº† 3ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ x 3ç§ä¸Šä¸‹æ–‡ç­–ç•¥ x 20ä¸ªé—®é¢˜ = 180æ¬¡å•ç‹¬çš„è¯•éªŒã€‚<br>

![](./test_set.jpg)


### Choosing a dataset(é€‰æ‹©æ•°æ®é›†)

First, we need to choose an appropriate dataset to try.<br>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„æ•°æ®é›†æ¥å°è¯•ã€‚<br>

We had a few guiding principles -<br>

æˆ‘ä»¬æœ‰ä¸€äº›æŒ‡å¯¼åŸåˆ™ -<br>

- **Representative:** Datasets in enterprises are often complex and this complexity isnâ€™t captured in many demo/sample datasets. We want to use a complicated database that has real-world use cases that contains real-world data.

å…·æœ‰ä»£è¡¨æ€§ã€‚ä¼ä¸šä¸­çš„æ•°æ®é›†é€šå¸¸å¾ˆå¤æ‚ï¼Œè¿™ç§å¤æ‚æ€§åœ¨è®¸å¤šæ¼”ç¤º/æ ·æœ¬æ•°æ®é›†ä¸­æ— æ³•æ•æ‰ã€‚æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ä¸€ä¸ªåŒ…å«çœŸå®ä¸–ç•Œæ•°æ®çš„å¤æ‚æ•°æ®åº“ï¼Œå®ƒæœ‰çœŸå®ä¸–ç•Œçš„ä½¿ç”¨æ¡ˆä¾‹ã€‚<br>

- **Accessible:** We also wanted that dataset to be publicly available.

å¯è®¿é—®æ€§ã€‚æˆ‘ä»¬è¿˜å¸Œæœ›è¿™ä¸ªæ•°æ®é›†èƒ½å¤Ÿå…¬å¼€è·å–ã€‚<br>

- **Understandable:** The dataset should be somewhat understandable to a wide audience - anything too niche or technical would be difficult to decipher.

æ˜“äºç†è§£ã€‚è¿™ä¸ªæ•°æ®é›†åº”è¯¥å¯¹å¹¿æ³›çš„å—ä¼—æœ‰ä¸€å®šçš„å¯ç†è§£æ€§â€”â€”ä»»ä½•è¿‡äºå°ä¼—æˆ–æŠ€æœ¯æ€§çš„å†…å®¹éƒ½ä¼šéš¾ä»¥è§£è¯»ã€‚<br>

Maintained. Weâ€™d prefer a dataset thatâ€™s maintained and updated properly, in reflection of a real database.<br>

ç»´æŠ¤è‰¯å¥½ã€‚æˆ‘ä»¬æ›´å–œæ¬¢ä¸€ä¸ªå¾—åˆ°é€‚å½“ç»´æŠ¤å’Œæ›´æ–°çš„æ•°æ®é›†ï¼Œè¿™åæ˜ äº†ä¸€ä¸ªçœŸå®çš„æ•°æ®åº“ã€‚<br>

âœ…A dataset that we found that met the criteria above was the Cybersyn SEC filings dataset, which is available for free on the Snowflake marketplace:<br>

âœ…æˆ‘ä»¬æ‰¾åˆ°çš„ä¸€ä¸ªç¬¦åˆä¸Šè¿°æ ‡å‡†çš„æ•°æ®é›†æ˜¯ Cybersyn SEC filings æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¯ä»¥åœ¨ Snowflake å¸‚åœºå…è´¹è·å–ï¼š<br>

```log
https://docs.cybersyn.com/our-data-products/economic-and-financial/sec-filings
```

### Choosing the questions

Next, we need to choose the questions. Here are some sample questions (see them all in this [file](https://github.com/vanna-ai/research/blob/main/data/questions_sec.csv)) -<br>

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©é—®é¢˜ã€‚è¿™é‡Œæœ‰ä¸€äº›ç¤ºä¾‹é—®é¢˜-<br>

1. How many companies are there in the dataset? - æ•°æ®é›†ä¸­æœ‰å¤šå°‘å®¶å…¬å¸ï¼Ÿ

2. What annual measures are available from the 'ALPHABET INC.' Income Statement? - 'ALPHABET INC.' æ”¶ç›Šè¡¨ä¸­æœ‰å“ªäº›å¹´åº¦æŒ‡æ ‡ï¼Ÿ

3. What are the quarterly 'Automotive sales' and 'Automotive leasing' for Tesla? - ç‰¹æ–¯æ‹‰æ¯å­£åº¦çš„â€œæ±½è½¦é”€å”®â€å’Œâ€œæ±½è½¦ç§Ÿèµâ€æƒ…å†µå¦‚ä½•ï¼Ÿ

4. How many Chipotle restaurants are there currently? - ç›®å‰æœ‰å¤šå°‘å®¶Chipotleé¤å…ï¼Ÿ

Now that we have the dataset + questions, weâ€™ll need to come up with the levers.<br>

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†æ•°æ®é›†å’Œé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦åˆ¶å®šç›¸åº”çš„ç­–ç•¥ã€‚<br>

### Choosing the prompt(é€‰æ‹©æç¤ºè¯­)

For the **prompt**, for this run, we are going to hold the prompt constant, though weâ€™ll do a follow up which varies the prompt.<br>

å¯¹äºè¿™æ¬¡çš„ **æç¤ºè¯­** ï¼Œæˆ‘ä»¬å°†ä¿æŒæç¤ºä¸å˜ï¼Œå°½ç®¡æˆ‘ä»¬ä¼šè¿›è¡Œåç»­çš„å˜åŠ¨ã€‚<br>

### Choosing the LLMs (Foundational models) - é€‰æ‹©LLMsï¼ˆåŸºç¡€æ¨¡å‹ï¼‰

For the LLMs to test, weâ€™ll try the following - å¯¹äºè¦æµ‹è¯•çš„LLMsï¼Œæˆ‘ä»¬å°†å°è¯•ä»¥ä¸‹å‡ ç§ -<br>

1. [Bison (Google)](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models?hl=zh-cn) - Bison is the version of [PaLM 2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) thatâ€™s available via GCP APIs. - Bisonï¼ˆè°·æ­Œï¼‰- Bisonæ˜¯é€šè¿‡GCP APIæä¾›çš„PaLM 2ç‰ˆæœ¬ã€‚

2. [GPT 3.5 Turbo (OpenAI)](https://platform.openai.com/docs/models/gpt-3-5-turbo) - GPT 3.5 until recently was the flagship OpenAI model despite 4 being available because of latency and cost benefits, and not a huge accuracy difference (well - weâ€™ll put that to the test) especially for basic tasks. - GPT 3.5 Turboï¼ˆOpenAIï¼‰- ç›´åˆ°æœ€è¿‘ï¼Œå°½ç®¡GPT 4å·²ç»æ¨å‡ºï¼Œä½†GPT 3.5ä»æ˜¯OpenAIçš„æ——èˆ°æ¨¡å‹ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå…¶åœ¨å»¶è¿Ÿå’Œæˆæœ¬ä¸Šçš„ä¼˜åŠ¿ï¼Œä¸”åœ¨å‡†ç¡®æ€§ä¸Šæ²¡æœ‰å·¨å¤§å·®å¼‚ï¼ˆå¥½å§ï¼Œæˆ‘ä»¬å°†å¯¹æ­¤è¿›è¡Œæµ‹è¯•ï¼‰ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŸºæœ¬ä»»åŠ¡ã€‚

3. [GPT 4 (OpenAI)](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) - The more advanced but less performant OpenAI model. GPT 4 is multi-modal, though we wonâ€™t be using that functionality. - GPT 4ï¼ˆOpenAIï¼‰- æ›´é«˜çº§ä½†æ€§èƒ½è¾ƒä½çš„OpenAIæ¨¡å‹ã€‚GPT 4æ˜¯å¤šæ¨¡æ€çš„ï¼Œå°½ç®¡æˆ‘ä»¬ä¸ä¼šä½¿ç”¨è¿™ä¸€åŠŸèƒ½ã€‚

4. [Llama 2 (Meta)](https://llama.meta.com/) - We really wanted to include an open source model - and the leading one is Metaâ€™s Llama 2. But our setup through [Replicate](https://replicate.com/meta/llama-2-70b-chat) quickly conked out, and we werenâ€™t able to fix it in time for publishing this. In our early runs, when it did work, we found performance to be mediocre at best. - Llama 2ï¼ˆMetaï¼‰- æˆ‘ä»¬çœŸçš„æƒ³è¦åŒ…æ‹¬ä¸€ä¸ªå¼€æºæ¨¡å‹ - æœ€ä¸»è¦çš„æ˜¯Metaçš„Llama 2ã€‚ä½†æ˜¯æˆ‘ä»¬é€šè¿‡Replicateçš„è®¾ç½®å¾ˆå¿«å‡ºç°æ•…éšœï¼Œæˆ‘ä»¬æ— æ³•åŠæ—¶ä¿®å¤ä»¥å‘å¸ƒè¿™ä¸ªã€‚åœ¨æˆ‘ä»¬çš„æ—©æœŸè¿è¡Œä¸­ï¼Œå½“å®ƒå·¥ä½œæ—¶ï¼Œæˆ‘ä»¬å‘ç°å…¶æ€§èƒ½æœ€å¤šä¹Ÿåªæ˜¯ä¸€èˆ¬ã€‚

### Choosing the context(é€‰æ‹©ä¸Šä¸‹æ–‡)

Finally, weâ€™ll have three types of **context**. Context refers to what we send to the LLM that helps give the LLM context on our specific dataset.<br>

æœ€åï¼Œæˆ‘ä»¬å°†æœ‰ä¸‰ç§ç±»å‹çš„ **ä¸Šä¸‹æ–‡** ã€‚ä¸Šä¸‹æ–‡æŒ‡çš„æ˜¯æˆ‘ä»¬å‘é€ç»™LLMçš„å†…å®¹ï¼Œå¸®åŠ©LLMç†è§£æˆ‘ä»¬ç‰¹å®šæ•°æ®é›†çš„ä¸Šä¸‹æ–‡ã€‚<br>

1. **Schema only:** We put the schema (using DDL) in the context window. - ä»…é™æ¨¡å¼ã€‚æˆ‘ä»¬åœ¨ä¸Šä¸‹æ–‡çª—å£ä¸­æ”¾ç½®æ¨¡å¼ï¼ˆä½¿ç”¨DDLï¼‰ã€‚<br>

> DDLæ˜¯æ•°æ®åº“é¢†åŸŸä¸­çš„ä¸€ä¸ªç¼©å†™ï¼Œä»£è¡¨â€œæ•°æ®å®šä¹‰è¯­è¨€â€ï¼ˆData Definition Languageï¼‰ã€‚ä¾‹å¦‚ CREATE ã€DROP è¯­å¥ã€‚

> SELECT è¯­å¥å±äº SQL ä¸­çš„ DMLï¼ˆæ•°æ®æ“çºµè¯­è¨€ï¼ŒData Manipulation Languageï¼‰éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ DDLï¼ˆæ•°æ®å®šä¹‰è¯­è¨€ï¼ŒData Definition Languageï¼‰ã€‚DML ä¸»è¦ç”¨äºæ•°æ®çš„æŸ¥è¯¢ã€æ’å…¥ã€æ›´æ–°å’Œåˆ é™¤æ“ä½œã€‚

ğŸ”¥ğŸ”¥ğŸ”¥å…¶å®å°±æ˜¯æä¾›è¡¨ç»“æ„ï¼Œæœ‰äº†è¡¨ç»“æ„æ‰èƒ½ç”Ÿæˆæ­£ç¡®çš„SQLè¯­å¥ã€‚<br>

2. **Static examples:** We put static example SQL queries in the context windows. - é™æ€ç¤ºä¾‹ã€‚æˆ‘ä»¬åœ¨ä¸Šä¸‹æ–‡çª—å£ä¸­æ”¾ç½®é™æ€ç¤ºä¾‹SQLæŸ¥è¯¢ã€‚

> é™æ€ç¤ºä¾‹å°±æ˜¯é¢„è®¾å¥½çš„SQLè¯­å¥ã€‚

3. **Contextually relevant examples:** Finally, we put the most relevant context (SQL / DDL / documentation) into the context window, finding it via a vector search based on embeddings. - ä¸Šä¸‹æ–‡ç›¸å…³ç¤ºä¾‹ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼ˆSQL / DDL / æ–‡æ¡£ï¼‰æ”¾å…¥ä¸Šä¸‹æ–‡çª—å£ï¼Œé€šè¿‡åŸºäºåµŒå…¥çš„å‘é‡æœç´¢æ‰¾åˆ°å®ƒã€‚

### é™æ€SQLä¸åŠ¨æ€SQLç¤ºä¾‹:

å‡è®¾ä½ æœ‰ä¸€ä¸ªç”µå•†ç½‘ç«™çš„æ•°æ®åº“ï¼Œä½ æƒ³æ ¹æ®ç”¨æˆ·çš„é€‰æ‹©åŠ¨æ€åœ°æŸ¥è¯¢å•†å“ä¿¡æ¯ã€‚<br>

**é™æ€ SQL ç¤ºä¾‹**:<br>

```sql
SELECT * FROM products WHERE category = 'Books';
```

è¿™ä¸ªæŸ¥è¯¢æ˜¯é™æ€çš„ï¼Œå› ä¸ºå®ƒæ€»æ˜¯æŸ¥è¯¢ç±»åˆ«ä¸º 'Books' çš„å•†å“ï¼Œä¸ä¼šæ ¹æ®ç”¨æˆ·çš„ä¸åŒé€‰æ‹©è€Œæ”¹å˜ã€‚<br>

**åŠ¨æ€ SQL ç¤ºä¾‹**:<br>

```sql
SELECT * FROM products WHERE category = ?;
```
åœ¨è¿™ä¸ªåŠ¨æ€æŸ¥è¯¢ä¸­ï¼Œé—®å· `?` æ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œå¯ä»¥åœ¨æ‰§è¡ŒæŸ¥è¯¢æ—¶è¢«å®é™…çš„ç”¨æˆ·è¾“å…¥æ›¿æ¢ï¼Œå¦‚ 'Electronics'ã€'Clothing' ç­‰ï¼Œè¿™æ ·æŸ¥è¯¢å°±å¯ä»¥æ ¹æ®ç”¨æˆ·çš„é€‰æ‹©æ¥å˜åŒ–äº†ã€‚<br>

åŠ¨æ€æŸ¥è¯¢é€šå¸¸ä½¿ç”¨ç¼–ç¨‹è¯­è¨€ç»“åˆ SQL å®ç°ï¼Œå¦‚åœ¨ Pythonã€Java ç­‰è¯­è¨€ä¸­ï¼Œé€šè¿‡ç¼–ç¨‹å°†ç”¨æˆ·çš„è¾“å…¥ä½œä¸ºå‚æ•°ä¼ é€’åˆ°æŸ¥è¯¢ä¸­ã€‚è¿™ç§æ–¹å¼è®©æŸ¥è¯¢æ›´åŠ çµæ´»å’Œåº”å¯¹å¤šå˜çš„éœ€æ±‚ã€‚<br>


## Using ChatGPT to generate SQL(ä½¿ç”¨ ChatGPT ç”Ÿæˆ SQL)

This section is a bit unfair to ChatGPT, but itâ€™s a helpful place to get started.<br>

è¿™ä¸€éƒ¨åˆ†å¯¹ ChatGPT æ¥è¯´æœ‰ç‚¹ä¸å…¬å¹³ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚<br>

Letâ€™s just take a question and ask ChatGPT to generate the SQL - æˆ‘ä»¬åªéœ€æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œè®© ChatGPT æ¥ç”Ÿæˆ SQL -<br>

### Prompt:

```log
Write Snowflake SQL to answer the following question - 

What are the quarterly revenues by business segment for Alphabet?

Return SQL only
```

åŒè¯­ç‰ˆ:<br>

```log
Write Snowflake SQL to answer the following question -  

ç¼–å†™ Snowflake SQL ä»¥å›ç­”ä»¥ä¸‹é—®é¢˜ -

What are the quarterly revenues by business segment for Alphabet?  

Alphabet çš„æ¯ä¸ªä¸šåŠ¡éƒ¨é—¨çš„å­£åº¦æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ

Return SQL only  

ä»…è¿”å› SQL
```

### Response:

Certainly, here's the SQL query:<br>

```sql
SELECT
  EXTRACT(QUARTER FROM date_column) AS quarter,
  business_segment,
  SUM(revenue) AS quarterly_revenue
FROM
  revenue_table
WHERE
  company_name = 'Alphabet'
GROUP BY
  quarter,
  business_segment
ORDER BY
  quarter,
  business_segment;

-- Replace date_column, business_segment, revenue, revenue_table, and adjust any other identifiers to match your specific database schema.
-- æ›¿æ¢ date_columnã€business_segmentã€revenueã€revenue_tableï¼Œå¹¶è°ƒæ•´ä»»ä½•å…¶ä»–æ ‡è¯†ç¬¦ä»¥åŒ¹é…æ‚¨çš„ç‰¹å®šæ•°æ®åº“æ¶æ„ã€‚
```

Inevitably, when we try to run this, we hit an error, because it made up the table name since we didn't provide it -<br>

ä¸å¯é¿å…åœ°ï¼Œå½“æˆ‘ä»¬å°è¯•è¿è¡Œè¿™æ®µä»£ç æ—¶ï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ï¼Œå› ä¸ºå®ƒç¼–é€ äº†è¡¨åï¼Œè€Œæˆ‘ä»¬å¹¶æ²¡æœ‰æä¾›å®ƒ -<br>

> æ•°æ®åº“ä¸­æ²¡æœ‰åä¸º revenue_table çš„è¡¨ã€‚

![](./sql_error.jpg)

Of course, we are being unfair to the LLMs - as magical as they are, they cannot (unfortunately? luckily?) possibly know whatâ€™s in our database - yet.<br>

å½“ç„¶ï¼Œæˆ‘ä»¬å¯¹LLMsä¸å…¬å¹³ - å°½ç®¡å®ƒä»¬éå¸¸ç¥å¥‡ï¼Œä½†å®ƒä»¬ä¸å¯èƒ½ï¼ˆä¸å¹¸åœ°ï¼Ÿå¹¸è¿åœ°ï¼Ÿï¼‰çŸ¥é“æˆ‘ä»¬æ•°æ®åº“ä¸­çš„å†…å®¹ - è‡³å°‘ç›®å‰æ˜¯è¿™æ ·ã€‚<br>

So letâ€™s hop into the tests where we give more context.<br>

æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬è¿›å…¥é‚£äº›æˆ‘ä»¬æä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„æµ‹è¯•ä¸­å»ã€‚<br>

### Using schema only:

First, we take the schema of the dataset and put it into the context window.<br>

é¦–å…ˆï¼Œæˆ‘ä»¬è·å–æ•°æ®è¡¨çš„æ¨¡å¼å¹¶å°†å…¶æ”¾å…¥ä¸Šä¸‹æ–‡çª—å£ã€‚<br>

This is usually what we've seen people do with ChatGPT or in tutorials.<br>

è¿™é€šå¸¸æ˜¯æˆ‘ä»¬åœ¨ä½¿ç”¨ChatGPTæˆ–åœ¨æ•™ç¨‹ä¸­çœ‹åˆ°çš„æ“ä½œæ–¹å¼ã€‚<br>

An example prompt may look like this (in reality we used the information schema because of how Snowflake shares work but this shows the principle) -<br>

ç¤ºä¾‹æç¤ºå¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼ˆå®é™…ä¸Šæˆ‘ä»¬ä½¿ç”¨äº†ä¿¡æ¯æ¨¡å¼ï¼Œå› ä¸ºè¿™æ˜¯Snowflakeå…±äº«çš„å·¥ä½œæ–¹å¼ï¼Œä½†è¿™å±•ç¤ºäº†åŸç†ï¼‰-<br>

```log
The user provides a question and you provide SQL. You will only respond with SQL code and not with any explanations.

Respond with only SQL code. Do not answer with any explanations -- just the code.

You may use the following DDL statements as a reference for what tables might be available.

CREATE TABLE Table1...

CREATE TABLE Table2...

CREATE TABLE Table3...
```

åŒè¯­ç‰ˆ:<br>

```log
The user provides a question and you provide SQL. You will only respond with SQL code and not with any explanations.  

ç”¨æˆ·æä¾›ä¸€ä¸ªé—®é¢˜ï¼Œä½ æä¾›SQLä»£ç ã€‚ä½ åªéœ€å›ç­”SQLä»£ç ï¼Œä¸éœ€è¦æä¾›ä»»ä½•è§£é‡Šã€‚

Respond with only SQL code. Do not answer with any explanations -- just the code.  

ä»…ä»¥SQLä»£ç å›åº”ã€‚ä¸è¦æä¾›ä»»ä½•è§£é‡Šâ€”â€”åªéœ€ä»£ç ã€‚

You may use the following DDL statements as a reference for what tables might be available.  

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹DDLè¯­å¥ä½œä¸ºå¯èƒ½å¯ç”¨çš„è¡¨çš„å‚è€ƒã€‚

CREATE TABLE Table1...  

CREATE TABLE Table2...  

CREATE TABLE Table3...  
```

The results were, in a word, terrible.â€¼ï¸<br>

ç»“æœå¯ä»¥ç”¨ä¸€ä¸ªè¯æ¥å½¢å®¹ï¼Œé‚£å°±æ˜¯ç³Ÿç³•ã€‚â€¼ï¸<br>

Of the 60 attempts (20 questions x 3 models), only two questions were answered correctly (both by GPT 4), for an abysmal accuracy rate of 3%.<br>

åœ¨60æ¬¡å°è¯•ä¸­ï¼ˆ20ä¸ªé—®é¢˜ x 3ä¸ªæ¨¡å‹ï¼‰ï¼Œåªæœ‰ä¸¤ä¸ªé—®é¢˜è¢«æ­£ç¡®å›ç­”ï¼ˆéƒ½æ˜¯ç”±GPT 4å®Œæˆï¼‰ï¼Œå‡†ç¡®ç‡æƒ¨æ·¡è‡³3%ã€‚<br>

Here are the two questions that GPT 4 managed to get right -<br>

ä»¥ä¸‹æ˜¯GPT 4æˆåŠŸå›ç­”æ­£ç¡®çš„ä¸¤ä¸ªé—®é¢˜ -<br>

What are the top 10 measure descriptions by frequency?<br>

é¢‘ç‡æœ€é«˜çš„å‰10ä¸ªåº¦é‡æè¿°æ˜¯ä»€ä¹ˆï¼Ÿ<br>

What are the distinct statements in the report attributes?<br>

æŠ¥å‘Šå±æ€§ä¸­çš„ä¸åŒå£°æ˜æœ‰å“ªäº›ï¼Ÿ<br>

![](./schema_only.jpg)

Itâ€™s evident that by just using the schema, we donâ€™t get close to meeting the bar of a helpful AI SQL agent, though it may be somewhat useful in being an analyst copilot.<br>

æ˜¾ç„¶ï¼Œä»…ä»…ä½¿ç”¨æ¨¡å¼ï¼Œæˆ‘ä»¬æ— æ³•è¾¾åˆ°ä¸€ä¸ªæœ‰ç”¨çš„AI SQLä»£ç†çš„æ ‡å‡†ï¼Œå°½ç®¡å®ƒåœ¨ä½œä¸ºåˆ†æå¸ˆå‰¯é©¾é©¶æ–¹é¢å¯èƒ½æœ‰äº›ç”¨å¤„ã€‚<br>

### Using SQL examples(ä½¿ç”¨ SQL ç¤ºä¾‹)

If we put ourselves in the shoes of a human whoâ€™s exposed to this dataset for the first time, in addition to the table definitions, theyâ€™d first look at the example queries to see how to query the database correctly.<br>

å¦‚æœæˆ‘ä»¬è®¾èº«å¤„åœ°ä¸ºç¬¬ä¸€æ¬¡æ¥è§¦è¿™ä¸ªæ•°æ®é›†çš„äººè€ƒè™‘ï¼Œé™¤äº†è¡¨çš„å®šä¹‰ï¼Œä»–ä»¬é¦–å…ˆåº”äº†è§£æ€æ ·è¿›è¡ŒæŸ¥è¯¢ï¼Œä»¥äº†è§£å¦‚ä½•æ­£ç¡®æŸ¥è¯¢æ•°æ®åº“ã€‚<br>

> æˆ‘ä»¬è®¾èº«å¤„åœ°çš„æŠŠè‡ªå·±å½“ä½œchatgptã€‚

These queries can give additional context not available in the schema - for example, which columns to use, how tables join together, and other intricacies of querying that particular dataset.<br>

è¿™äº›æŸ¥è¯¢å¯ä»¥æä¾›è¡¨ç»“æ„ä¸­æ— æ³•è·å¾—çš„é¢å¤–ä¸Šä¸‹æ–‡ â€”â€” ä¾‹å¦‚ï¼Œä½¿ç”¨å“ªäº›åˆ—ï¼Œè¡¨å¦‚ä½•è¿æ¥åœ¨ä¸€èµ·ï¼Œä»¥åŠæŸ¥è¯¢è¯¥ç‰¹å®šæ•°æ®é›†çš„å…¶ä»–å¤æ‚æ€§ã€‚<br>

Cybersyn, as with other data providers on the Snowflake marketplace, provides a few (in this case 3) example queries in their documentation.<br>

ä¸ Snowflake å¸‚åœºä¸Šçš„å…¶ä»–æ•°æ®æä¾›å•†ä¸€æ ·ï¼ŒCybersyn åœ¨å…¶æ–‡æ¡£ä¸­æä¾›äº†ä¸€äº›ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º 3 ä¸ªï¼‰ç¤ºä¾‹æŸ¥è¯¢ã€‚<br> 

Letâ€™s include these in the context window.<br>

è®©æˆ‘ä»¬å°†è¿™äº›åŒ…å«åœ¨ä¸Šä¸‹æ–‡çª—å£ä¸­ã€‚<br>

By providing just those 3 example queries, we see substantial improvements to the correctness of the SQL generated.<br>

ä»…æä¾›è¿™ 3 ä¸ªç¤ºä¾‹æŸ¥è¯¢ï¼Œæˆ‘ä»¬å°±çœ‹åˆ°äº†ç”Ÿæˆçš„ SQL çš„æ­£ç¡®æ€§æœ‰äº†æ˜¾è‘—æé«˜ã€‚<br>

However, this accuracy greatly varies by the underlying LLM.<br>

ç„¶è€Œï¼Œè¿™ç§å‡†ç¡®æ€§å› åº•å±‚ LLM è€Œå¼‚ã€‚<br>

It seems that GPT-4 is the most able to generalize the example queries in a way that generates the most accurate SQL.<br>

ä¼¼ä¹ GPT-4 æœ€èƒ½æ¦‚æ‹¬ç¤ºä¾‹æŸ¥è¯¢ï¼Œä»è€Œç”Ÿæˆæœ€å‡†ç¡®çš„ SQLã€‚<br>


## VANNA_API_KEY ç”¨åœ¨å“ªé‡Œï¼Ÿ

Vanna.ai ä½¿ç”¨ API å¯†é’¥ä¸»è¦æ˜¯ä¸ºäº†è®¿é—®æ‰˜ç®¡åœ¨ Vanna æœåŠ¡å™¨ä¸Šçš„ç»„ä»¶ï¼Œæ¯”å¦‚è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–å‘é‡å­˜å‚¨ã€‚å¦‚æœä½ å·²ç»æœ‰äº†è‡ªå·±çš„è¯­è¨€æ¨¡å‹å’Œå‘é‡å­˜å‚¨ï¼Œå¯ä»¥ä¸ä½¿ç”¨ API å¯†é’¥æ¥è¿è¡Œ Vannaã€‚æ­¤å¤–ï¼Œå¦‚æœä½ åªæ˜¯æƒ³å¿«é€Ÿå¼€å§‹è€Œæ— éœ€æŠ•å…¥æˆæœ¬ï¼ŒVanna æä¾›äº†å…è´¹çš„ API å¯†é’¥ã€‚<br>

æ€»çš„æ¥è¯´ï¼ŒAPI å¯†é’¥éœ€è¦çš„éƒ¨åˆ†ä¸»è¦æ¶‰åŠåˆ°ä¸€äº›æ‰˜ç®¡æœåŠ¡æˆ–ç‰¹å®šåŠŸèƒ½ï¼Œè€Œ Vanna çš„åŸºç¡€ä½¿ç”¨å’Œè®¸å¤šåŠŸèƒ½ä»ç„¶ä¿æŒå¼€æºå’Œå…è´¹ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºå¦‚ä½•è·å–å’Œä½¿ç”¨ Vanna API å¯†é’¥çš„ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£æä¾›çš„è¯¦ç»†è¯´æ˜ã€‚<br>